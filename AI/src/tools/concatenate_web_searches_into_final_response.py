import os
from google import genai
from google.genai import types
from dotenv import load_dotenv
from datetime import datetime
import re
import json

# Load environment variables
load_dotenv()

# Get API key from environment
GEMINI_API_KEY = os.getenv("GEMINI_KEY")
CURRENT_DATE = datetime.now().strftime("%Y-%m-%d")

def load_concatenation_system_prompt(response_style: str = "detailed"):
    """Load the system prompt for concatenating web search results based on response style"""
    try:
        # Get the directory where this script is located
        script_dir = os.path.dirname(os.path.abspath(__file__))
        # Go up to src directory, then to instructions/concatenate_responses_get_final_response
        
        # Select the appropriate prompt file based on response style
        if response_style == "compact":
            prompt_filename = "concatenate_web_searches_compact.txt"
        else:
            prompt_filename = "concatenate_web_searches.txt"  # Default to detailed
        
        prompts_path = os.path.join(script_dir, "..", "instructions", "concatenate_responses_get_final_response", prompt_filename)
        prompts_path = os.path.normpath(prompts_path)
        
        with open(prompts_path, "r", encoding="utf-8") as file:
            system_prompt = file.read().strip()
            # Replace the date placeholder with current date
            system_prompt = system_prompt.replace("{CURRENT_DATE}", CURRENT_DATE)
            print(f"âœ… Loaded {response_style} response style prompt from {prompt_filename}")
            return system_prompt
    except FileNotFoundError:
        print(f"âŒ Error: {prompt_filename} file not found at {prompts_path}")
        print(f"Expected location: src/instructions/concatenate_responses_get_final_response/{prompt_filename}")
        return None
    except Exception as e:
        print(f"âŒ Error loading concatenation system prompt: {e}")
        return None

def load_rag_context_file(domain_name: str, rag_context_path: str = "rag_context") -> str:
    """
    Load RAG context file for a specific domain (dfmt.ro or timpark.ro)
    
    Args:
        domain_name: The domain name (e.g., 'dfmt.ro', 'timpark.ro')
        rag_context_path: The path to RAG context files directory
    
    Returns:
        Content of the RAG context file or None if not found
    """
    try:
        # Get the directory where this script is located
        script_dir = os.path.dirname(os.path.abspath(__file__))
        # Go up to src directory, then to the specified RAG context path
        rag_context_file_path = os.path.join(script_dir, "..", rag_context_path, domain_name)
        rag_context_file_path = os.path.normpath(rag_context_file_path)
        
        if os.path.exists(rag_context_file_path):
            with open(rag_context_file_path, "r", encoding="utf-8") as file:
                content = file.read().strip()
                print(f"âœ… Loaded RAG context for {domain_name} ({len(content)} characters)")
                return content
        else:
            print(f"âš ï¸ RAG context file not found for domain {domain_name} at {rag_context_file_path}")
            return None
    except Exception as e:
        print(f"âŒ Error loading RAG context for {domain_name}: {e}")
        return None

def extract_relevant_rag_contexts(selected_domains: list, rag_config: dict = None) -> dict:
    """
    Extract RAG contexts for domains that have corresponding RAG files
    
    Args:
        selected_domains: List of domains selected by trusted_sites_search
        rag_config: RAG configuration from agent_config.json
    
    Returns:
        Dictionary mapping domain names to their RAG context content
    """
    # Check if RAG context is enabled
    if not rag_config or not rag_config.get('use_rag_context', False):
        print(f"ğŸ“ RAG context integration is disabled in configuration")
        return {}
    
    # Get configured RAG domains and path
    configured_rag_domains = rag_config.get('rag_domains', ['dfmt.ro', 'timpark.ro'])
    rag_context_path = rag_config.get('rag_context_path', 'rag_context')
    
    relevant_contexts = {}
    
    if not selected_domains:
        return relevant_contexts
    
    print(f"\nğŸ” Checking for RAG contexts in selected domains: {selected_domains}")
    print(f"ğŸ“š Configured RAG domains: {configured_rag_domains}")
    print(f"ğŸ“ RAG context path: {rag_context_path}")
    
    for domain in selected_domains:
        # Check if this domain is in the configured RAG domains list
        if domain in configured_rag_domains:
            rag_content = load_rag_context_file(domain, rag_context_path)
            if rag_content:
                relevant_contexts[domain] = rag_content
                print(f"âœ… Added RAG context for {domain}")
    
    if relevant_contexts:
        print(f"ğŸ“š Total RAG contexts loaded: {len(relevant_contexts)} domains")
    else:
        print(f"ğŸ“ No RAG contexts found for the selected domains")
    
    return relevant_contexts

def create_user_input_for_gemini(
    original_question: str,
    reformulated_query: str = None,
    regular_web_search_results: str = None,
    trusted_sites_search_results: dict = None,
    rag_contexts: dict = None
) -> str:
    """
    Create the structured user input for Gemini to process all the information
    Now includes RAG context integration
    """
    
    user_input = f"""**ÃNTREBAREA ORIGINALÄ‚ A UTILIZATORULUI:**
{original_question}

"""
    
    if reformulated_query:
        user_input += f"""**INTEROGAREA REFORMULATÄ‚:**
{reformulated_query}

"""
    else:
        user_input += f"""**INTEROGAREA REFORMULATÄ‚:**
Nu a fost utilizatÄƒ reformularea - s-a folosit Ã®ntrebarea originalÄƒ.

"""
    
    if regular_web_search_results:
        user_input += f"""**REZULTATELE CÄ‚UTÄ‚RII WEB GENERALE (Perplexity):**
{regular_web_search_results}

"""
    else:
        user_input += f"""**REZULTATELE CÄ‚UTÄ‚RII WEB GENERALE (Perplexity):**
CÄƒutarea web generalÄƒ nu a fost activatÄƒ sau nu a produs rezultate.

"""
    
    if trusted_sites_search_results and trusted_sites_search_results.get('success'):
        domains = trusted_sites_search_results.get('selected_domains', [])
        search_text = trusted_sites_search_results.get('search_results', '')
        
        user_input += f"""**REZULTATELE CÄ‚UTÄ‚RII PE SITE-URI DE ÃNCREDERE (Perplexity pe domenii selectate de Gemini):**
Domenii guvernamentale romÃ¢neÈ™ti selectate ({len(domains)} total):
{', '.join(domains)}

Rezultatele cÄƒutÄƒrii pe site-urile de Ã®ncredere:
{search_text}

"""
    else:
        user_input += f"""**REZULTATELE CÄ‚UTÄ‚RII PE SITE-URI DE ÃNCREDERE (Perplexity pe domenii selectate de Gemini):**
CÄƒutarea pe site-uri de Ã®ncredere nu a fost activatÄƒ sau nu a produs rezultate.

"""
    
    # NEW: Add RAG context section if available
    if rag_contexts and len(rag_contexts) > 0:
        user_input += f"""**CONTEXT RAG DETALIAT DIN BAZELE DE DATE LOCALE:**
Am identificat informaÈ›ii detaliate din bazele de date locale pentru urmÄƒtoarele domenii selectate:

"""
        for domain, context_content in rag_contexts.items():
            user_input += f"""**Context pentru {domain.upper()}:**
{context_content}

"""
        
        user_input += f"""**INSTRUCÈšIUNI PENTRU UTILIZAREA CONTEXTULUI RAG:**
- Contextul RAG de mai sus conÈ›ine informaÈ›ii oficiale extrase È™i procesate din site-urile guvernamentale romÃ¢neÈ™ti
- Aceste informaÈ›ii sunt foarte actuale È™i detaliate pentru domeniile specifice ({', '.join(rag_contexts.keys())})
- FolosiÈ›i acest context pentru a oferi informaÈ›ii foarte precise, specifice È™i detaliate Ã®n rÄƒspunsul final
- Contextul RAG completeazÄƒ rezultatele cÄƒutÄƒrii pe site-uri de Ã®ncredere cu detalii suplimentare oficiale
- PrioritizaÈ›i informaÈ›iile din contextul RAG pentru aspectele tehnice, proceduri exacte, taxe specifice, È™i detalii administrative
- IntegraÈ›i natural aceste informaÈ›ii Ã®n rÄƒspunsul final fÄƒrÄƒ sÄƒ menÈ›ionaÈ›i explicit "contextul RAG"

"""
    
    return user_input

def concatenate_web_searches_into_final_response(
    original_question: str,
    reformulated_query: str = None,
    regular_web_search_results: str = None,
    trusted_sites_search_results: dict = None,
    # Gemini parameters
    temperature: float = 0.1,
    max_tokens: int = 15000,
    model: str = "gemini-2.5-flash-preview-04-17",
    # RAG configuration parameters
    rag_config: dict = None,
    # Response style configuration
    response_style: str = "detailed",
    # Output parameters
    save_to_file: bool = True
) -> str:
    """
    Concatenate and synthesize web search results into a final comprehensive response
    Now enhanced with configurable RAG context integration and response style selection
    
    Args:
        original_question: The user's original question
        reformulated_query: The reformulated query (if used)
        regular_web_search_results: Results from regular web search
        trusted_sites_search_results: Results dict from trusted sites search
        temperature: Controls randomness (0.0-1.0, lower = more focused)
        max_tokens: Maximum tokens to generate
        model: Gemini model to use
        rag_config: RAG configuration dict from agent_config.json
        response_style: Response style ("detailed" or "compact")
        save_to_file: Whether to save the final response to file
    
    Returns:
        The final synthesized response
    """
    
    print(f"\nğŸ”§ FINAL RESPONSE GENERATION - DETAILED DEBUGGING")
    print("=" * 60)
    
    # Validate API key
    if not GEMINI_API_KEY:
        print("âŒ Error: GEMINI_KEY not found in environment variables")
        return None
    
    print(f"âœ… API Key loaded: {GEMINI_API_KEY[:10]}...{GEMINI_API_KEY[-10:]}")
    
    # Load system prompt based on response style
    system_prompt = load_concatenation_system_prompt(response_style)
    if not system_prompt:
        print("âŒ Failed to load system prompt")
        return None
    
    print(f"âœ… System prompt loaded successfully ({len(system_prompt)} characters)")
    print(f"ğŸ“ System prompt preview: {system_prompt[:200]}...")
    
    # NEW: Extract RAG contexts based on selected domains and configuration
    rag_contexts = {}
    if trusted_sites_search_results and trusted_sites_search_results.get('success'):
        selected_domains = trusted_sites_search_results.get('selected_domains', [])
        rag_contexts = extract_relevant_rag_contexts(selected_domains, rag_config)
    
    print(f"\nğŸ”§ Final Response Generation Configuration:")
    print(f"   Model: {model}")
    print(f"   Temperature: {temperature}")
    print(f"   Max Tokens: {max_tokens}")
    print(f"   Response Style: {response_style}")
    print(f"   Original Question: '{original_question}'")
    print(f"   Has Reformulated Query: {'Yes' if reformulated_query else 'No'}")
    print(f"   Has Regular Search Results: {'Yes' if regular_web_search_results else 'No'}")
    print(f"   Has Trusted Sites Results: {'Yes' if trusted_sites_search_results and trusted_sites_search_results.get('success') else 'No'}")
    print(f"   RAG Context Enabled: {'Yes' if rag_config and rag_config.get('use_rag_context', False) else 'No'}")
    print(f"   RAG Contexts Available: {len(rag_contexts)} domains ({', '.join(rag_contexts.keys()) if rag_contexts else 'None'})")
    
    # Create structured user input with RAG context integration
    user_input = create_user_input_for_gemini(
        original_question=original_question,
        reformulated_query=reformulated_query,
        regular_web_search_results=regular_web_search_results,
        trusted_sites_search_results=trusted_sites_search_results,
        rag_contexts=rag_contexts  # NEW: Pass RAG contexts
    )
    
    print(f"\nğŸ“„ User input created successfully ({len(user_input)} characters)")
    print(f"ğŸ“ User input preview (first 300 chars):")
    print("-" * 40)
    print(user_input[:300] + "...")
    print("-" * 40)
    
    try:
        # Initialize the new Gen AI client
        print(f"\nğŸ”Œ Initializing Gemini API client...")
        client = genai.Client(api_key=GEMINI_API_KEY)
        print("âœ… Gemini API client configured for final response generation")
        
        # Prepare content using the new SDK structure
        print(f"\nğŸ“¦ Preparing content structure...")
        contents = [
            types.Content(
                role="user",
                parts=[
                    types.Part.from_text(text=user_input),
                ],
            ),
        ]
        print(f"âœ… Content structure prepared with {len(contents)} message(s)")
        print(f"ğŸ“Š First content role: {contents[0].role}")
        print(f"ğŸ“Š First content parts count: {len(contents[0].parts)}")
        print(f"ğŸ“Š First part text length: {len(contents[0].parts[0].text)} characters")
        
        # Generate final response with system instruction in config
        print(f"\nâš™ï¸ Preparing generation config...")
        generate_content_config = types.GenerateContentConfig(
            system_instruction=system_prompt,
            temperature=temperature,
            max_output_tokens=max_tokens,
            response_mime_type="text/plain"
        )
        print(f"âœ… Generation config prepared:")
        print(f"   ğŸ“Š Temperature: {generate_content_config.temperature}")
        print(f"   ğŸ“Š Max tokens: {generate_content_config.max_output_tokens}")
        print(f"   ğŸ“Š MIME type: {generate_content_config.response_mime_type}")
        print(f"   ğŸ“Š System instruction length: {len(generate_content_config.system_instruction)} characters")
        
        print(f"\nğŸš€ Making API call to {model}...")
        print(f"ğŸ“¡ Sending request with:")
        print(f"   ğŸ”¹ Model: {model}")
        print(f"   ğŸ”¹ Contents: {len(contents)} messages")
        print(f"   ğŸ”¹ Total input size: ~{len(user_input) + len(system_prompt)} characters")
        
        # Check if input might be too large
        total_input_size = len(user_input) + len(system_prompt)
        if total_input_size > 30000:
            print(f"âš ï¸ Input size ({total_input_size} chars) is quite large, this might cause timeout issues")
        
        try:
            response = client.models.generate_content(
                model=model,
                contents=contents,
                config=generate_content_config
            )
            print(f"âœ… API call completed successfully!")
        except Exception as api_error:
            print(f"âŒ API call failed: {api_error}")
            print(f"ğŸ”„ Trying with reduced input size...")
            
            # Try with a smaller user input
            if len(user_input) > 8000:
                print(f"ğŸ“‰ Reducing user input from {len(user_input)} to ~8000 characters...")
                truncated_input = user_input[:8000] + "\n\n[INPUT TRUNCATED DUE TO SIZE LIMITATIONS]"
                
                contents_fallback = [
                    types.Content(
                        role="user",
                        parts=[
                            types.Part.from_text(text=truncated_input),
                        ],
                    ),
                ]
                
                try:
                    response = client.models.generate_content(
                        model=model,
                        contents=contents_fallback,
                        config=generate_content_config
                    )
                    print(f"âœ… Fallback API call with reduced input completed successfully!")
                except Exception as fallback_error:
                    print(f"âŒ Fallback API call also failed: {fallback_error}")
                    raise fallback_error
            else:
                raise api_error
        
        # Debug response structure
        print(f"\nğŸ” RESPONSE DEBUGGING:")
        print(f"   ğŸ“Š Response type: {type(response)}")
        print(f"   ğŸ“Š Response object: {response}")
        
        if hasattr(response, 'text'):
            print(f"   ğŸ“Š Has .text attribute: Yes")
            print(f"   ğŸ“Š .text value: {repr(response.text)}")
            print(f"   ğŸ“Š .text type: {type(response.text)}")
            if response.text:
                print(f"   ğŸ“Š .text length: {len(response.text)}")
            else:
                print(f"   âš ï¸ .text is None or empty")
        else:
            print(f"   âŒ No .text attribute found")
        
        if hasattr(response, 'candidates'):
            print(f"   ğŸ“Š Has .candidates attribute: Yes")
            print(f"   ğŸ“Š Candidates count: {len(response.candidates) if response.candidates else 0}")
            if response.candidates:
                for i, candidate in enumerate(response.candidates):
                    print(f"   ğŸ“Š Candidate {i}: {candidate}")
                    if hasattr(candidate, 'content'):
                        print(f"   ğŸ“Š Candidate {i} content: {candidate.content}")
                    if hasattr(candidate, 'finish_reason'):
                        print(f"   ğŸ“Š Candidate {i} finish_reason: {candidate.finish_reason}")
            else:
                print(f"   âš ï¸ .candidates is None or empty")
        else:
            print(f"   âŒ No .candidates attribute found")
        
        if hasattr(response, '__dict__'):
            print(f"   ğŸ“Š Response attributes: {list(response.__dict__.keys())}")
            for key, value in response.__dict__.items():
                if key not in ['text', 'candidates']:
                    print(f"   ğŸ“Š {key}: {value}")
        
        # Extract response text
        if response and response.text:
            final_response = response.text
            print(f"âœ… Final response generated successfully! ({len(final_response)} characters)")
            print(f"ğŸ“ Response preview (first 200 chars):")
            print("-" * 40)
            print(final_response[:200] + "...")
            print("-" * 40)
            
            # Save to file if requested
            if save_to_file:
                filename = save_final_response_to_file(
                    original_question=original_question,
                    final_response=final_response,
                    generation_metadata={
                        "model": model,
                        "temperature": temperature,
                        "max_tokens": max_tokens,
                        "response_style": response_style,
                        "has_reformulated_query": bool(reformulated_query),
                        "has_regular_search": bool(regular_web_search_results),
                        "has_trusted_search": bool(trusted_sites_search_results and trusted_sites_search_results.get('success')),
                        "rag_contexts_used": list(rag_contexts.keys()) if rag_contexts else []  # NEW: Track RAG usage
                    }
                )
                print(f"ğŸ’¾ Final response saved to: {filename}")
            
            return final_response
        else:
            print("âŒ No text found in response")
            print("ğŸ” Attempting to extract text from alternative sources...")
            
            # Try alternative extraction methods
            extracted_text = None
            if hasattr(response, 'candidates') and response.candidates:
                for i, candidate in enumerate(response.candidates):
                    print(f"   ğŸ” Checking candidate {i}...")
                    if hasattr(candidate, 'content') and candidate.content:
                        if hasattr(candidate.content, 'parts') and candidate.content.parts:
                            for j, part in enumerate(candidate.content.parts):
                                print(f"   ğŸ” Checking candidate {i}, part {j}...")
                                if hasattr(part, 'text') and part.text:
                                    extracted_text = part.text
                                    print(f"   âœ… Found text in candidate {i}, part {j}!")
                                    break
                            if extracted_text:
                                break
                        elif hasattr(candidate.content, 'text') and candidate.content.text:
                            extracted_text = candidate.content.text
                            print(f"   âœ… Found text in candidate {i} content!")
                            break
                    if extracted_text:
                        break
            
            if extracted_text:
                print(f"âœ… Successfully extracted text via alternative method! ({len(extracted_text)} characters)")
                return extracted_text
            else:
                print("âŒ Could not extract text from any source")
                return None
        
    except Exception as e:
        print(f"âŒ Error making Gemini API call for final response: {e}")
        print(f"ğŸ” Exception type: {type(e)}")
        import traceback
        print(f"ğŸ“„ Full traceback:")
        print(traceback.format_exc())
        return None

def save_final_response_to_file(original_question, final_response, generation_metadata, filename=None):
    """Save the final synthesized response to a file"""
    if filename is None:
        # Create filename from question
        clean_query = re.sub(r'[^\w\s-]', '', original_question)
        clean_query = re.sub(r'\s+', '_', clean_query)
        clean_query = clean_query[:50].strip('_')
        timestamp = datetime.now().strftime("%m_%d_%H_%M")
        filename = f"FINAL_{clean_query}_{timestamp}.txt"
    
    try:
        with open(filename, "w", encoding="utf-8") as file:
            file.write("="*80 + "\n")
            file.write("RÄ‚SPUNS FINAL SINTETIZAT - AGENT CIVIC ROMÃ‚N\n")
            file.write("="*80 + "\n\n")
            file.write(f"Ãntrebarea OriginalÄƒ: {original_question}\n")
            file.write(f"Data ProcesÄƒrii: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            file.write("-"*80 + "\n\n")
            file.write("CONFIGURAÈšIA SINTEZEI:\n")
            file.write(f"   Model: {generation_metadata['model']}\n")
            file.write(f"   Temperature: {generation_metadata['temperature']}\n")
            file.write(f"   Max Tokens: {generation_metadata['max_tokens']}\n")
            file.write(f"   Response Style: {generation_metadata.get('response_style', 'detailed')}\n")
            file.write(f"   Reformulare folositÄƒ: {'Da' if generation_metadata['has_reformulated_query'] else 'Nu'}\n")
            file.write(f"   CÄƒutare web regulatÄƒ: {'Da' if generation_metadata['has_regular_search'] else 'Nu'}\n")
            file.write(f"   CÄƒutare site-uri de Ã®ncredere: {'Da' if generation_metadata['has_trusted_search'] else 'Nu'}\n")
            # NEW: Track RAG context usage
            rag_contexts_used = generation_metadata.get('rag_contexts_used', [])
            file.write(f"   Context RAG folosit: {'Da' if rag_contexts_used else 'Nu'}")
            if rag_contexts_used:
                file.write(f" ({', '.join(rag_contexts_used)})")
            file.write("\n")
            file.write("-"*80 + "\n\n")
            file.write("RÄ‚SPUNSUL FINAL SINTETIZAT:\n")
            file.write("-"*30 + "\n")
            file.write(final_response)
            file.write("\n\n" + "="*80 + "\n")
            file.write("NOTA: Acest rÄƒspuns a fost generat prin sinteza inteligentÄƒ a rezultatelor\n")
            file.write("din cÄƒutÄƒri multiple, prioritizÃ¢nd sursele oficiale guvernamentale romÃ¢neÈ™ti")
            if rag_contexts_used:
                file.write(f"\nÈ™i integrÃ¢nd context RAG detaliat din: {', '.join(rag_contexts_used)}")
            file.write(".\n")
            file.write("="*80 + "\n")
        
        print(f"ğŸ’¾ Final response saved to: {filename}")
        return filename
    except Exception as e:
        print(f"âŒ Error saving final response: {e}")
        return None

if __name__ == "__main__":
    print("=" * 80)
    print("FINAL RESPONSE GENERATION - CONCATENATE WEB SEARCHES + RAG CONTEXT")
    print("=" * 80)
    print("ğŸ”„ This tool synthesizes results from multiple search tools:")
    print("   1ï¸âƒ£  Query reformulation results")
    print("   2ï¸âƒ£  Regular web search results")  
    print("   3ï¸âƒ£  Trusted government sites search results")
    print("   4ï¸âƒ£  ğŸ†• RAG context integration (configurable)")
    print("   5ï¸âƒ£  Final synthesized response")
    print("=" * 80)
    
    # Test with sample data
    TEST_QUESTION = "taxe locuinta timisoara 2025"
    TEST_REFORMULATED = "Care sunt taxele È™i impozitele locale pentru locuinÈ›e Ã®n TimiÈ™oara pentru anul 2025, cum se calculeazÄƒ, unde se plÄƒtesc È™i care sunt termenele?"
    TEST_REGULAR_SEARCH = "Pentru taxele de locuinÈ›Äƒ Ã®n TimiÈ™oara Ã®n 2025... [rezultate cÄƒutare regulatÄƒ]"
    TEST_TRUSTED_SEARCH = {
        "success": True,
        "selected_domains": ["dfmt.ro", "evpers.primariatm.ro", "timpark.ro"],  # Including dfmt.ro for testing
        "search_results": "Taxele locale pentru locuinÈ›e Ã®n TimiÈ™oara... [rezultate oficiale]"
    }
    
    # Load actual agent configuration to test real settings
    try:
        script_dir = os.path.dirname(os.path.abspath(__file__))
        config_path = os.path.join(script_dir, "..", "agent_config.json")
        config_path = os.path.normpath(config_path)
        
        with open(config_path, "r", encoding="utf-8") as f:
            agent_config = json.load(f)
        
        # Extract RAG configuration from agent config
        RAG_CONFIG = agent_config.get("final_response_generation", {}).get("rag_context", {
            "use_rag_context": False,
            "rag_domains": ["dfmt.ro", "timpark.ro"],
            "rag_context_path": "rag_context"
        })
        
        print(f"\nğŸ“š Loading RAG configuration from agent_config.json:")
        print(f"   ğŸ”§ RAG Context Enabled: {RAG_CONFIG.get('use_rag_context', False)}")
        print(f"   ğŸ”§ RAG Domains: {RAG_CONFIG.get('rag_domains', [])}")
        print(f"   ğŸ”§ RAG Context Path: {RAG_CONFIG.get('rag_context_path', 'rag_context')}")
        
    except Exception as e:
        print(f"âš ï¸  Could not load agent_config.json, using fallback RAG configuration: {e}")
        # Fallback RAG Configuration for testing
        RAG_CONFIG = {
            "use_rag_context": True,
            "rag_domains": ["dfmt.ro", "timpark.ro"],
            "rag_context_path": "rag_context"
        }
    
    print(f"\nğŸ¯ Testing final response generation with RAG context for: '{TEST_QUESTION}'")
    print("-" * 80)
    
    # Generate final response
    result = concatenate_web_searches_into_final_response(
        original_question=TEST_QUESTION,
        reformulated_query=TEST_REFORMULATED,
        regular_web_search_results=TEST_REGULAR_SEARCH,
        trusted_sites_search_results=TEST_TRUSTED_SEARCH,
        rag_config=RAG_CONFIG,  # Use actual agent configuration
        response_style="detailed",
        save_to_file=True
    )
    
    if result:
        print(f"\nğŸ‰ FINAL RESPONSE WITH RAG CONTEXT GENERATED SUCCESSFULLY!")
        print("-" * 60)
        print(f"Response length: {len(result)} characters")
        print(f"First 200 characters: {result[:200]}...")
    else:
        print(f"\nâŒ FINAL RESPONSE GENERATION FAILED")
    
    print("\n" + "=" * 80)
    print("âœ¨ This tool now supports configurable RAG contexts!")
    print("   ğŸ“š Configure RAG integration in agent_config.json:")
    print("   ğŸ“ 'use_rag_context': true/false - Enable/disable RAG")
    print("   ğŸ“ 'rag_domains': ['dfmt.ro', 'timpark.ro'] - Domains to check")
    print("   ğŸ“ 'rag_context_path': 'rag_context' - Path to RAG files")
    print("   ğŸ”§ When enabled and matching domains are selected by trusted_sites_search,")
    print("   ğŸ”§ their corresponding detailed RAG context files are loaded automatically")
    print("=" * 80)
